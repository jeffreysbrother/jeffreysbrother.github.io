---
title: "A/B Tests"
---

Let's imagine that we've established that the result of an A/B test will produce statistically significant results, according to industry research. This hypothetical site or application receives *a lot* of traffic, so we don't need to worry about misleading disparities emerging from some meager data set.

The test in question might amount to us directing half the traffic to one page, while the other half is directed at a modified version of that page. Perhaps a call to action is red in one case, yellow in another. Perhaps the headline is changed from "View Products Now" to "Find Out What You're Missing". Maybe the woman holding the bottle of alcohol on the modified page is younger, slightly more attractive, and less WASPy.

We are told that a large sample size secures statistical significance, and that statistical significance has the ability to ensure that our decision to pick the "prevailing" CTA, headline, or image (the variation that resulted in the most conversions, for example) will be a well-informed and practical one.

Let's pull out of this particular hypothetical daydream and dip into another. Assume that we have good reason to believe that, despite having a large sample size and a killer headline and raw data that appears to point to the superior variant, the test was worthless. Perhaps a few months later the resulting data shows that the previous test had no effect on the conversion rate. Or, perhaps we eventually realize that the 9% increase in conversion was a result of factors entirely unrelated to our test (the recall of a competitor's product, fluctuations in the economy, the seasonal nature of the product, the modified load-time of the site after a PHP update). In both cases, the marketing manager might STILL consider this worthless test to be valuable, since the client was coerced in paying $12,000 for it. I get it.

But we should be careful not to equivocate over the term "value". I am considering the value that comes from a test being legitimate, as opposed to the value that comes from monetary gain.

Without reason to suspect that an updated button will improve visibility, why should we *ever* think that a new color alone will influence an individual's desire to buy? I completely understand the utility of re-design and optimization in terms of site performance and copywriting. Yes, it might be true that a particular design is better able to hold the user's attention, establish trust or legitimacy, or effectively communicate the utility of some product. Large scale split tests might be useful in some cases. The kind of A/B test I am intending to attack is the one that assumes that an isolated, minute modification will somehow influence the decision of the user (which I have seen a lot of, recently). In my opinion, tests like this are worthless and usually the product of uninformed, dogmatic, or dishonest marketing.

Apart from admitting that a split test of
